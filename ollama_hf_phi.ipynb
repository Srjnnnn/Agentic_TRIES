{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to load tokenizer for model phi3:medium: Incorrect path_or_model_id: 'phi3:medium'. Please provide either the path to a local folder or the repo_id of a model on the Hub.. Loading default tokenizer instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"In night's embrace, she softly glows,\\nA silver orb that ebbs and flows.\\nShe whispers tales of ancient lore,\\nAnd bathes the world in gentle light.\\n\\nHer craters hold secrets deep,\\nOf cosmic dances and celestial sweep.\\nIn her eternal cycle she spins,\\nGuiding sailors through darkened seas.\\n\\nShe waxes full with graceful poise,\\nA beacon for the dreaming toys.\\nHer crescent smile brings hope anew,\\nAs stars twinkle in admiration too.\\n\\nThe moon, our constant friend above,\\nIn her celestial dance we find love.\\nFor though she wanes and fades away,\\nShe'll rise again with dawn of day.\"}]\n",
      "[{'generated_text': \"Here's the corrected version of your code. The issue was that you were trying to call `list()` on an integer value which is not allowed. Instead, we should use indexing with square brackets []:\\n\\n```python\\nmy_list = [0, 1, e]\\n\\nfor i in range(4):\\n    print(my_list[i])\\n```\\n\\nThis code will now correctly iterate over the list and print each element.\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers.agents.llm_engine import HfApiEngine, MessageRole, get_clean_message_list\n",
    "\n",
    "openai_role_conversions = {  # Keep this for message cleaning\n",
    "    MessageRole.TOOL_RESPONSE: \"user\",\n",
    "}\n",
    "\n",
    "class OllamaHfApiWrapper(HfApiEngine):\n",
    "    def __init__(self, model_name, temperature, base_url=\"http://localhost:11434/\"):\n",
    "        super().__init__(model_name) \n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def query(self, payload):\n",
    "        messages = payload.get(\"inputs\", [])\n",
    "        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)\n",
    "        data = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False, # Set to false for non-streaming responses\n",
    "            \"options\": {\"temperature\": self.temperature},\n",
    "        }\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.post(self.base_url + \"/api/chat\", json=data, headers=headers, timeout=120)\n",
    "\n",
    "            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "            response_json = response.json()\n",
    "            ## llm_output = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            llm_output = response_json[\"message\"][\"content\"].strip()\n",
    "            return [{\"generated_text\": llm_output}] # HfApiEngine expects a list of dictionaries\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during Ollama request: {e}\")\n",
    "            return [{\"generated_text\": \"\"}]  # Return empty string in case of error\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Error parsing Ollama response: {e}. Raw response: {response.text}\")\n",
    "            return [{\"generated_text\": \"\"}]\n",
    "\n",
    "# Example usage:\n",
    "ollama_engine = OllamaHfApiWrapper(model_name=\"phi3:medium\", temperature=0, base_url=\"http://localhost:11434/\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a short poem about the moon.\"}]\n",
    "\n",
    "response = ollama_engine.query({\"inputs\": messages})\n",
    "print(response)\n",
    "\n",
    "code = \"\"\"py\n",
    "list=[0, 1, 2]\n",
    "\n",
    "for i in range(4):\n",
    "    print(list(i))\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"I have some code that creates a bug: please debug it and return the final code with static range specification, code: {}\".format(code)}]\n",
    "\n",
    "response = ollama_engine.query({\"inputs\": messages})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
