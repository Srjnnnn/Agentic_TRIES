{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: git+https://github.com/huggingface/transformers.git#egg=transformers[agents]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git#egg=transformers[agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install markdownify duckduckgo-search spaces gradio-tools langchain langchain-community langchain-huggingface faiss-cpu --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a6fc7e6294482391ef0c063dedd168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import HfApiEngine\n",
    "\n",
    "llm_model = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "llm_engine = HfApiEngine(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import ReactCodeAgent, ReactJsonAgent, ManagedAgent\n",
    "from transformers.agents.search import DuckDuckGoSearchTool, VisitWebpageTool\n",
    "\n",
    "web_agent = ReactJsonAgent(tools=[DuckDuckGoSearchTool(), VisitWebpageTool()], llm_engine=llm_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_web_agent = ManagedAgent(\n",
    "    agent=web_agent,\n",
    "    name=\"search\",\n",
    "    description=\"Runs web searches for you. Give it your query as an argument.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2647/2647 [00:27<00:00, 97.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]}) for doc in knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split docs and keep only unique ones\n",
    "print(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_docs):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[new_doc.page_content] = True\n",
    "            docs_processed.append(new_doc)\n",
    "\n",
    "print(\"Embedding documents...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "huggingface_doc_vector_db = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import Tool\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, vectordb: VectorStore, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.vectordb.similarity_search(\n",
    "            query,\n",
    "            k=7,\n",
    "        )\n",
    "\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_doc_retriever_tool = RetrieverTool(huggingface_doc_vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GITHUB_ACCESS_TOKEN = os.environ.get(\"GITHUB_PERSONAL_TOKEN\")\n",
    "\n",
    "print(GITHUB_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(repo=\"huggingface/peft\", access_token=GITHUB_ACCESS_TOKEN, include_prs=False, state=\"all\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_issues_vector_db = FAISS.from_documents(chunked_docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_issues_retriever_tool = RetrieverTool(peft_issues_vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_agent = ReactJsonAgent(\n",
    "    tools=[huggingface_doc_retriever_tool, peft_issues_retriever_tool],\n",
    "    llm_engine=llm_engine,\n",
    "    max_iterations=4,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_retriever_agent = ManagedAgent(\n",
    "    agent=retriever_agent,\n",
    "    name=\"retriever\",\n",
    "    description=\"Retrieves documents from the knowledge base for you that are close to the input query. Give it your query as an argument. The knowledge base includes Hugging Face documentation and PEFT issues.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://sergiopaniego-promptist.hf.space ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since `api_name` was not defined, it was automatically set to the first avilable API: `/predict`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import load_tool, CodeAgent\n",
    "\n",
    "prompt_generator_tool = Tool.from_space(\n",
    "    \"sergiopaniego/Promptist\", name=\"generator_tool\", description=\"Optimizes user input into model-preferred prompts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're loading a tool from the Hub from None. Please make sure this is a source that you trust as the code within that tool will be executed on your machine. Always verify the code of the tools that you load. We recommend specifying a `revision` to ensure you're loading the code that you have checked.\n",
      "TextToImageTool implements a different description in its configuration and class. Using the tool configuration description.\n"
     ]
    }
   ],
   "source": [
    "image_generation_tool = load_tool(\"m-ric/text-to-image\", cache=False)\n",
    "image_generation_agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_image_generation_agent = ManagedAgent(\n",
    "    agent=image_generation_agent,\n",
    "    name=\"image_generation\",\n",
    "    description=\"Generates images from text prompts. Give it your prompt as an argument.\",\n",
    "    additional_prompting=\"\\n\\nYour final answer MUST BE only the generated image location.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_agent = ReactCodeAgent(\n",
    "    tools=[],\n",
    "    llm_engine=llm_engine,\n",
    "    managed_agents=[managed_web_agent, managed_retriever_agent, managed_image_generation_agent],\n",
    "    additional_authorized_imports=[\"time\", \"datetime\", \"PIL\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mImprove this prompt, then generate an image of it.\n",
      "You have been provided with these initial arguments: {'prompt': 'A guy wearing a suit and riding a horse.'}.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: improve task be_involved in this generation, so the provided argument 'prompt is: 'A guy wearing a suit and riding a horse.' I will generate an image of this prompt using the `image_generation` function.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generation\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mprompt\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mA guy wearing a suit and riding a horse\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[31;20mCode execution failed due to the following error:\n",
      "ManagedAgent.__call__() missing 1 required positional argument: 'request'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1196, in step\n",
      "    result = self.python_evaluator(\n",
      "        code_action,\n",
      "    ...<3 lines>...\n",
      "        authorized_imports=self.authorized_imports,\n",
      "    )\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 902, in evaluate_python_code\n",
      "    result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 741, in evaluate_ast\n",
      "    return evaluate_assign(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 322, in evaluate_assign\n",
      "    result = evaluate_ast(assign.value, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 746, in evaluate_ast\n",
      "    return evaluate_call(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 439, in evaluate_call\n",
      "    output = func(*args, **kwargs)\n",
      "TypeError: ManagedAgent.__call__() missing 1 required positional argument: 'request'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1215, in step\n",
      "    raise AgentExecutionError(error_msg)\n",
      "transformers.agents.agents.AgentExecutionError: Code execution failed due to the following error:\n",
      "ManagedAgent.__call__() missing 1 required positional argument: 'request'\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: The error indicates that the `image_generation` tool requires a 'request' parameter, not a 'prompt' argument. I will provide the improved prompt as a request to the `image_generation` tool.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generation\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mrequest\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mA gentleman in a tailored suit skillfully riding a majestic horse\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'image_generation'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "A gentleman in a tailored suit skillfully riding a majestic horse\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\n",
      "\n",
      "Your final answer MUST BE only the generated image location.\u001b[0m\n",
      "\u001b[31;20mCode execution failed due to the following error:\n",
      "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: 6Csvan0ec5qe3XOQ8Amyb)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1196, in step\n",
      "    result = self.python_evaluator(\n",
      "        code_action,\n",
      "    ...<3 lines>...\n",
      "        authorized_imports=self.authorized_imports,\n",
      "    )\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 902, in evaluate_python_code\n",
      "    result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 741, in evaluate_ast\n",
      "    return evaluate_assign(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 322, in evaluate_assign\n",
      "    result = evaluate_ast(assign.value, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 746, in evaluate_ast\n",
      "    return evaluate_call(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 439, in evaluate_call\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1260, in __call__\n",
      "    output = self.agent.run(full_task, **kwargs)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 676, in run\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\"], **additional_args)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n",
      "    response = self.generate(messages, stop_sequences, grammar)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/llm_engine.py\", line 190, in generate\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 892, in chat_completion\n",
      "    data = self.post(model=model_url, json=payload, stream=stream)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 306, in post\n",
      "    hf_raise_for_status(response)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: 6Csvan0ec5qe3XOQ8Amyb)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1215, in step\n",
      "    raise AgentExecutionError(error_msg)\n",
      "transformers.agents.agents.AgentExecutionError: Code execution failed due to the following error:\n",
      "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-72B-Instruct/v1/chat/completions (Request ID: 6Csvan0ec5qe3XOQ8Amyb)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: Let process is to generate an image of a gentleman wearing a suit and riding a horse using the `image_generator` tool. Please to generate the final image.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generation\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mprompt\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144ma gentleman in a tailored suit, elegantly riding a majestic horse\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[31;20mCode execution failed due to the following error:\n",
      "ManagedAgent.__call__() missing 1 required positional argument: 'request'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1196, in step\n",
      "    result = self.python_evaluator(\n",
      "        code_action,\n",
      "    ...<3 lines>...\n",
      "        authorized_imports=self.authorized_imports,\n",
      "    )\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 902, in evaluate_python_code\n",
      "    result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 741, in evaluate_ast\n",
      "    return evaluate_assign(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 322, in evaluate_assign\n",
      "    result = evaluate_ast(assign.value, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 746, in evaluate_ast\n",
      "    return evaluate_call(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 439, in evaluate_call\n",
      "    output = func(*args, **kwargs)\n",
      "TypeError: ManagedAgent.__call__() missing 1 required positional argument: 'request'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 856, in direct_run\n",
      "    self.step(step_log_entry)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 1215, in step\n",
      "    raise AgentExecutionError(error_msg)\n",
      "transformers.agents.agents.AgentExecutionError: Code execution failed due to the following error:\n",
      "ManagedAgent.__call__() missing 1 required positional argument: 'request'\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: The error indicates that the `image_generation` tool requires a 'request' parameter, not a 'prompt' argument. I will provide the improved prompt as a request to the `image_generation` tool again, while making sure to use 'request' in the function call.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generation\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mrequest\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144ma gentleman in a tailored suit, elegantly riding a majestic horse\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[32;20;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mYou're a helpful agent named 'image_generation'.\n",
      "You have been submitted this task by your manager.\n",
      "---\n",
      "Task:\n",
      "a gentleman in a tailored suit, elegantly riding a majestic horse\n",
      "---\n",
      "You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n",
      "\n",
      "Your final_answer WILL HAVE to contain these parts:\n",
      "### 1. Task outcome (short version):\n",
      "### 2. Task outcome (extremely detailed version):\n",
      "### 3. Additional context (if relevant):\n",
      "\n",
      "Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n",
      "And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\n",
      "\n",
      "Your final answer MUST BE only the generated image location.\u001b[0m\n",
      "\u001b[33;1m=== Agent thoughts:\u001b[0m\n",
      "\u001b[0mThought: I will use the following tool: `image_generator` to generate an image based on the provided prompt.\u001b[0m\n",
      "\u001b[33;1m>>> Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;60;03m# Generating a detailed and high-quality image\u001b[39;00m\n",
      "\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mimage_generator\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mprompt\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144ma gentleman in a tailored suit, elegantly riding a majestic horse, high-res, photorealistic\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;60;03m# Preparing the detailed answer\u001b[39;00m\n",
      "\u001b[38;5;7mfinal_answer\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7manswer\u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7mimage\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[31;20mError in execution: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell (Request ID: 1qvGKDZtpsW3q5BzMzo_Z)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s). Be sure to provide correct code.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/agents.py\", line 701, in run\n",
      "    output = self.python_evaluator(\n",
      "        code_action,\n",
      "    ...<3 lines>...\n",
      "        authorized_imports=self.authorized_imports,\n",
      "    )\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 902, in evaluate_python_code\n",
      "    result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 741, in evaluate_ast\n",
      "    return evaluate_assign(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 322, in evaluate_assign\n",
      "    result = evaluate_ast(assign.value, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 746, in evaluate_ast\n",
      "    return evaluate_call(expression, state, static_tools, custom_tools)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/python_interpreter.py\", line 439, in evaluate_call\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/transformers/agents/tools.py\", line 183, in __call__\n",
      "    outputs = self.forward(*args, **kwargs)\n",
      "  File \"/home/sercan/.cache/huggingface/modules/transformers_modules/m-ric/text-to-image/27c6979e2ca9e26f4b7aab85e244664e9d86304f/tool.py\", line 15, in forward\n",
      "    return self.client.text_to_image(prompt)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 2447, in text_to_image\n",
      "    response = self.post(**payload, model=model, task=\"text-to-image\")\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 306, in post\n",
      "    hf_raise_for_status(response)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"/home/sercan/anaconda3/envs/general/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, str(e), response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell (Request ID: 1qvGKDZtpsW3q5BzMzo_Z)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s)\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20m\u001b[0m\n",
      "\u001b[33;1mLast output from code snippet:\u001b[0m\n",
      "\u001b[32;20mError in execution: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell (Request ID: 1qvGKDZtpsW3q5BzMzo_Z)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s). Be sure to provide correct code.\u001b[0m\n",
      "\u001b[32;20;1mFinal answer:\u001b[0m\n",
      "\u001b[32;20mError in execution: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell (Request ID: 1qvGKDZtpsW3q5BzMzo_Z)\n",
      "\n",
      "Model too busy, unable to get response in less than 60 second(s). Be sure to provide correct code.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = manager_agent.run(\n",
    "    \"Improve this prompt, then generate an image of it.\", prompt=\"A guy wearing a suit and riding a horse.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
